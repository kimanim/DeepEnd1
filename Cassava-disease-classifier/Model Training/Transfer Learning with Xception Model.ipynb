{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: Using Pre-Trained Xception Network\n",
    "## Using Xception for Feature Extraction (With Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains Transfer learning code for extracting features using a pre-trained Xception Network. This code assumes that our custom dataset is small, and, because of that, we use Keras data augmentation methods to augment the dataset by applying a set of random transformations to each of the original images. Data augmentation enables us to generate more training data from our small dataset of training samples by applying these random transformations to the dataset such that the model never sees the same image twice during training (this is analogous to increasing the number of the images). Data augmentation is a form of regularization that helps us avoid overfitting (due to having very few samples to learn data representation from) and enables the trained model to generalize better (to unseen data). This code should ONLY be run on a GPU as it is too expensive to run on a CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure:\n",
    "\n",
    "1) Add the convolutional base model to a Sequential model\n",
    "2)Flatten the convolutional base outputs (before feeding them to the densely-connected classifier\n",
    "3) Add a densely connected classifier on top of the flattened convolutional base model \n",
    "3) Freeze the convolutional base model \n",
    "4) Compile the model \n",
    "5) Train the model [end-to-end training with data augmentation] \n",
    "6) Save the trained model"
       ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries/packages \n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "\n",
    "# Import the necessary libraries\n",
    "from keras.applications.xception import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import merge, Input\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Convolutional Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the feature extraction part of the Xception network trained on ImageNet (convolutional base)\n",
    "\n",
    "Xception_conv_base = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "print(Xception_conv_base.summary())\n",
    "print(Xception_conv_base.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the custom dataset\n",
    "# Use Python's pathlib library to enable the use of foward slashes even in Windows\n",
    "# For details on pathlib, See: https://bit.ly/2HTbaEY \n",
    "\n",
    "dataset_path = Path('Path/to/custom/dataset/directory')\n",
    "data_dir_list = os.listdir(dataset_path)\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the Input Data (Imagery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of the image paths in our custom dataset and randomly \n",
    "# shuffle them to allow for easy training and testing splits via\n",
    "# array slicing during training time\n",
    "\n",
    "imagePaths = sorted(list(paths.list_images(dataset_path)))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "print(len(imagePaths))\n",
    "\n",
    "# Loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image,pre-process the image, and store it in the data list\n",
    "    img= image.load_img(imagePath, target_size= (299,299))\n",
    "    x = image.img_to_array(img)\n",
    "    #print(x.shape)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "#   print('Input image shape:', x.shape)\n",
    "    data.append(x)\n",
    "#   print(data[0])\n",
    "    # extract the class labels from the image paths then encode the\n",
    "    # labels, assuming # that our image paths has the format shown below:\n",
    "    # /path/to/dataset/{class}/{image}.jpg\n",
    "    label = imagePath.split(os.path.sep)[-2] \n",
    "    # store the label in the labels list\n",
    "    labels.append(label)\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the Data and Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data and labels into NumPy array\n",
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "\n",
    "#Change the shape of the data to(number of images, 299, 299, 3)\n",
    "data=np.rollaxis(data,1,0)\n",
    "print (data.shape)\n",
    "data=data[0]\n",
    "print (data.shape)\n",
    "\n",
    "labels = np.array(labels)\n",
    "print(labels.shape)\n",
    "\n",
    "#Encode the labels (from integers to vectors)\n",
    "le = LabelEncoder()  \n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing (validation)\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "test_size=0.2, random_state=42)\n",
    "\n",
    "# optionqlly check the shapes of the data splits\n",
    "print('Shape of training images is:', trainX.shape)\n",
    "print('Shape of validation images is:', testX.shape)\n",
    "print('Shape of training labels is:', trainY.shape)\n",
    "print('Shape of validation labels is:', testY.shape)\n",
    "\n",
    "#get the length (number of samples) of the training and validation data\n",
    "nTrain = len(trainX)\n",
    "nVal = len(testX)\n",
    "\n",
    "print('Total number of training images/samples is:', nTrain)\n",
    "print('Total number of validation images/samples is:',nVal)\n",
    "\n",
    "# convert the labels from integers to vectors (one-hot encoding)\n",
    "# This is necessary because the .flow method does not accept the class_mode parameter\n",
    "# as is the case with .flow_from_directory method\n",
    "\n",
    "# One-Hot Encoding of the labels\n",
    "trainY = to_categorical(trainY, num_classes=3)\n",
    "testY = to_categorical(testY, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a custom Densely-Connected classifier on Top of the Convolutional Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a custom classification layer\n",
    "num_classes = 3\n",
    "\n",
    "#Create custom Xception Classifier Model \n",
    "model = models.Sequential()\n",
    "model.add(Xception_conv_base) \n",
    "model.add(layers.Flatten()) # Flatten convolutional base\n",
    "model.add(layers.Dense(256, activation='relu')) \n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an ImageDataGenerator Object Class to Load the Images and Use the flow Method to Generate Batches of Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data augmentation configuration to prevent overfitting due to our small custom dataset\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "#Create the validation data generator (validation dataset is not augmented)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Set the batch size\n",
    "batch_size = 30\n",
    "\n",
    "# Create the Python image generators to generate batches of images and labels\n",
    "train_generator = train_datagen.flow(trainX, trainY, \n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle)\n",
    "val_generator = val_datagen.flow(testX, testY, \n",
    "                                batch_size=batch_size)                       \n",
    "                                # No shuffling for the validation set                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the Convolutional Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following gives a sense of the change in the number of trainable weights after freezing the convolutional base of the Xception model\n",
    "print('This is the number of trainable weights before freezing the convolutional base:', len(model.trainable_weights))\n",
    "\n",
    "Xception_conv_base.trainable = False\n",
    "\n",
    "print('This is the number of trainable weights after freezing the convolutional base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nTrain // batch_size,\n",
    "                    epochs=20,\n",
    "                    validation_data= val_generator,\n",
    "                    validation_steps=nVal // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model architecture and weights to HDF5\n",
    "\n",
    "model.save(Path('Path/to/where/to/save/the/model/model_name.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results of Training and Validation Accuracies & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will plot the curves of loss and accuracy during training\n",
    "\n",
    "# Get values that were specified during model compilation which are saved in the \n",
    "# history object\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Get the number of epochs from the values in the 'acc' list\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Training and validation accuracy plot [Accuracy at each epoch]\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')       \n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')  \n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.figure()\n",
    "\n",
    "# Training and validation loss plot [Loss at each epoch]\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepLearning]",
   "language": "python",
   "name": "conda-env-DeepLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
